%! TEX root = ../msc.tex
\chapter{Introduction}
\label{ch:basics}
\epigraph{I am fascinated by numbers}{
\citeauthor{baron-cohenAutismSpectrumQuotientAQ2001}}
%\cite{baron-cohenAutismSpectrumQuotientAQ2001}}

Was hier useful werden kann:

%Nielsen: \cite{nielsenQuantumComputationQuantum2010}
%
%Stabilizer Formalism, Gottesman PhD thesis: \cite{gottesmanStabilizerCodesQuantum1997}
%
%Algorithm for simulating stabilizer circuits:
%\cite{aaronsonImprovedSimulationStabilizer2004}

stabilizer lecture notes: \cite{arabLectureNotesQuantum2024}

%Entanglement with Stabilizers: \cite{fattalEntanglementStabilizerFormalism2004}
%
%Aaronson's quantum information theory I and II lecture notes:
%\cite{aaronsonIntroductionQuantumInformation,aaronsonIntroductionQuantumInformationa}.
%This one's a huge find!
%
%another huge find! gottesman lecture notes on quantum error correction:
%\cite{gottesmanSurvivingQuantumComputer2024}
%
%Zee Group Theory in a nutshell: \cite{zeeGroupTheoryNutshell2016}

This chapter serves to familiarize the reader with the core concepts relevant
to this thesis. We will first introduce the stabilizer formalism, as it will
later enable us to perform efficient numerical experiments on a classical
computer. We then provide a general introduction to the field of entanglement
transitions and go over some important examples. 
\newpage
\section{The stabilizer formalism}\label{sec:stab-basics}
In quantum computation and quantum computing we are exposed to a multitude of
obstacles to overcome. Two obvious ones are the issues of quantum error
correction and the exponential space complexity of genuine quantum simulations
on classical computers. While it seems almost absurd to suggest that these two
problems would be dealt with by the same formalism, we nevertheless want to
wash out doubts about this fact in the present section and present an
introduction to the stabilizer formalism.

Although it was originally introduced as an approach to quantum error
correction
\cite{nielsenQuantumComputationQuantum2010,gottesmanStabilizerCodesQuantum1997,gottesmanClassQuantumErrorcorrecting1996,gottesmanSurvivingQuantumComputer2024}, the notion of
stabilizers has become synonymous with efficient classical simulability in the
world of quantum information theory
\cite{nielsenQuantumComputationQuantum2010,aaronsonImprovedSimulationStabilizer2004,aaronsonIntroductionQuantumInformation,aaronsonIntroductionQuantumInformationa,fisherRandomQuantumCircuits2023}.

As the stabilizer formalism relies heavily on group theoretic arguments, we
will first begin by giving a brief overview of the foundational algebra.
\subsection{Basic notions of group theory}
Group theory is one of the most important concepts from algebra in the toolset
of theoretical physicists.
From classifying crystalline structures
\cite{ashcroftSolidStatePhysics1978},
(gauge) symmetries in the standard model \textcolor{orange}{(cite some yang
mills stuff)}, to the
classification of (topological) phase transitions \textcolor{orange}{(cite
nicolai phd?)}, group
theory perpetually permeates theoretical physiscs.

The stabilizer formalism makes no exception. It is a clever application of
group theory, allowing for a more compact representation of quantum states
compared to the state vector. We therefore introduce necessary prerequesites of
group theory needed for the stabilizer formalism and its role in this thesis,
starting with the notion of a group.

A group $G$ is a collection of some particulars $g$ that can be composed
according to some convention. To be called a group, the set of entities
$\{g_i\}$ and the operation\footnote{In the discussion on groups and group
  theory, the words composition, group operation, and multiplication are used
interchangeably} with which we compose them, need to obey a certain
ruleset \cite{zeeGroupTheoryNutshell2016}. This ruleset, called the group axioms, reads as follows
\cite{stroppelHoehereMathematik12023}.

\begin{defn}[Group]\label{defn:group}
  A group $G$ is a non-empty set equipped with a binary operation (here denoted
  with $\cdot$) that satisfies
  \begin{description}
    \item[Associativity] The group operation is associative, i.e.
      $$\forall a,b,c \in G:\ (a\cdot b) \cdot c = a \cdot
      (b \cdot c)$$
    \item[Identity element] The group contains an identity element, which does
      nothing with respect to composition, i.e.
      $$\exists I\in G \ \forall g \in G : I\cdot g = g \cdot I = g$$
    \item[Inverse element] Each group element has a unique element associated
      to it that when composed with it gives the identity. In other words,
      $$\forall g \in G \ \exists g^{-1} \in G : g g^{-1} = g^{-1} g = I $$
  \end{description}
\end{defn}

Note that we do not require the group elements to commute with respect to
multiplication. Groups that satisfy commutativity for all their elements are
called \emph{abelian} groups.

Numerous different mathematical objects and concepts can fall under the
umbrella of group theory.\footnote{One can even describe the Rubik's Cube
puzzle in the language of group theory} Although we will discuss specific
groups in greater detail later, we do not want to fail to mention some other
important groups appearing all across physics. There are the rotation groups
$SO(n)$ and unitary groups $U(n)$ (both in $n$ dimensions), the permutation
group of $n$ elements $S_n$ and the group of square roots of $1$ under
multiplication, $\mathbb{Z}_2 = \{1, -1\}$.  The first two are examples of
continuous groups, while the others are discrete. Note that discrete and
infinite are not mutually exclusive. The group $(\mathbb{Z}, +)$ is an example
of a discrete group with infinite number of elements.
The number of elements in a finite group $G$ is
called the \emph{order} of a group, which we write as $\mathrm{ord}(G)$.

For larger finite groups, it can become cumbersome to keep track of all the
elements and their relation to each other. Luckily, there is a way we can
condense all the information to construct the whole group in its
\emph{presentation}, also known as its \emph{generating set}
\cite{zeeGroupTheoryNutshell2016}. The elements of
such a generating set are referred to as \emph{generators}.
\begin{defn}[Generating set and generators]\label{defn:generators}
  Let $G$ be a finite group and $S$ a subset of $G$. $S$ is called a generating
  set of $G$ if all elements in $G$ can be obtained from (possibly repeated)
  multiplication of elements in the generating set. Generating sets are denoted
  by angled braces, such that we write
  \begin{align}
    S = \langle g_1, \ldots, g_n \rangle, \quad g_1,\ldots, g_n \in G
  .\end{align}
  The $g_i$ are called \emph{generators} of $G$. The trivial group $\left(
  \{I\}, \cdot \right)$ is generated by the empty set.
\end{defn}

While there are also generators of continuous or infinite groups, they take on a
fundamentally different form compared to the finite counterpart. As an
example of a generating set in the finite case, consider the group of fourth roots of $1$
with multiplication, $Z_4 = \left( \{\pm 1,\pm i\}, \cdot \right)$. It is easy
to verify that the subset $S = \{i\}$ of the group uniquely reproduces all of
the elements in $Z_4$, by taking powers of $i$.

In the process of constructing a generating set we are generally faced with two
restrictions. The first is the fact that the entire group needs to emerge from
the multiplications of generators. We cannot simply choose arbitrary group
elements. $S=\{-1\}\subset Z_4$ would be a perfectly fine set of generators for
$\mathbb{Z}_2$, but not $Z_4$. Next, we ideally want to have the least number
of generators possible to build up the rest of the group. This restriction is
one we impose on ourselves rather than one imposed on us by necessity. Choosing
$S=\{-1, i, -i\}$ as generators also recovers $Z_4$, however, we have already
seen that $g=-1$ and $g=-i$ are redundant in this context.
\Cref{thm:maxsize-generators} gives an upper bound on the number of generators
needed to generate finite groups.

\begin{thm}[\cite{nielsenQuantumComputationQuantum2010}]\label{thm:maxsize-generators} 
  The minimum size of a generating set for a finite group $G$ of generators is
  at most $\log_2(\mathrm{ord}(G))$.
\end{thm}

%\begin{proof} We prove the theorem by induction over the size of 
%
%  As base case we have Let $G$ be a finite group with minimal generating set
%  $S_n = \langle g_1, \ldots, g_n\rangle.$
%\end{proof}

With \cref{thm:maxsize-generators} we have that for $Z_4$, choosing a
generating set with $3$ elements should be cause for concern, since we would
need $2$ at most. However, we also saw that $Z_4$ is special in that way, since
we only needed one generator. A way of quantifying this quality is the
\emph{rank} of a group.  It is defined as the size of the smallest generating
set of $G$ and is denoted by $\abs{G}$. Thus, for the example of $Z_4$ we have
$\abs{Z_4}=1$. 

When discussing subsets of groups, one naturally arising concept is the notion
of subgroups. Suppose we take some set of elements $\{g\}$ forming a group $G$
under multiplication and take a subset $\{h\}$ thereof. If the subset also
forms a group $H$, we call it a subgroup of $G$ and write $H \leq G$.

\begin{defn}[Subgroup \cite{zeeGroupTheoryNutshell2016}]\label{defn:subgroup}
  A subgroup $H$ of $G$, written as $H \leq G$, is a non-empty subset $H$ of $G$, which forms
  a group under the same group operation as $G$. 
\end{defn}

Going back to some of the previous examples, we can consider $SO(2)$, rotations
along the unit circle, as rotations on the equator of a unit sphere. We can
consequently take $SO(2)$ as a subgroup of the rotation group of the unit
sphere $SO(3)$. The group of permutations of $m \leq n$ elements is just the
group of permutations of $n$ elements, where $n-m$ elements are left invariant,
and as such $S_m \leq S_n$. Note that for any group $G$ we have $G \leq G$ and
$(\{I\}, \cdot) \leq G$, where $\left( \{I\}, \cdot \right)$ is the trivial
group containing only the identity. These two special cases are referred to as
\emph{trivial subgroups}. Subgroups that are not trivial are called
\emph{proper subgroups} denoted by $H<G$.

Before introducing another important family of subgroups, we define a special
kind of operation known as \emph{conjugation}. If $h,g\in G$, the conjugate of
$h$ with respect to $g$ is $g^{-1} h g$.\footnote{This operation is
colloquially referred to as \enquote{sandwiching} $h$ with $g$.}
We can not only perform this operation on individual group elements, but also
subgroups of $G$. 
Consider the proper subgroup $H<G$ with elements $\{h_i\}$. 
%Another important family of subgroups are \emph{invariant} subgroups. Let $H$
%be a group of elements $\{h\}$ with $H<G$.
If we take any element $g \in G
\setminus H$, we can arrive at another subgroup of $G$ by conjugating all
elements of the
subgroup $H$ with $g$, written as
\begin{align}
  g^{-1}Hg = \{ g^{-1} h g \ \mid \ h \in H \}
.\end{align}
In general, the two subgroups $H$ and $g^{-1}Hg$ need not be the same. However,
if there are some $g$ which leave $H$ invariant under conjugation, we say that
these $g$ \emph{normalize} $H$. A collection of these normalizing elements can
be compiled together to form yet another subgroup of $G$, called the
normalizer.
\begin{defn}[Normalizer \cite{gottesmanSurvivingQuantumComputer2024}]\label{defn:normalizer}
  Let $G$ be a group and $H < G$ a proper subgroup of $G$. The normalizer of
  $H$ in $G$ is the subgroup of $G$ that leaves $H$ invariant under
  conjugation, i.e.
  \begin{align}
    N_G(H) = \{ g \in G \mid \ g^{-1} H g = H \}
  .\end{align}
\end{defn}
In the special case of every element of $G$ normalizing $H$, that is $N_G(H) =
G$, $H$ is called an \emph{invariant subgroup} of $G$
\cite{zeeGroupTheoryNutshell2016}. In the next section we
will introduce an important example of a finite group and its normalizer. 
%However,
%if, for an arbitrary choice of $g$, they are the same, then $H$ is called
%\emph{invariant subgroup} of $G$.
%\begin{defn}[Invariant subgroup]\label{defn:normal-subgroup}
%  Let $G, H$ be groups with $H\lt G$. We call $H$ an invariant (or normal)
%  subgroup of $G$ if
%  \begin{align}
%    \forall g \in G, \forall h \in H: \ g^{-1} h g \in H
%  .\end{align}
%\end{defn}
\subsection{The Pauli group and Clifford gates}

Consider the Pauli matrices with the identity,
\begin{align}
  \sigma_0 = I = \mqty(\pmat{0}),\quad \sigma_x = \mqty(\pmat{1}), \quad \sigma_y =
  \mqty(\pmat{2}), \quad \text{and}\quad \sigma_z = \mqty(\pmat{3})
.\end{align}
These well-known matrices are both hermitian and unitary, and consequently
square to the identity. For the latter three of them one can show that they
satisfy the following commutation and anticommutation relations,
\begin{equation}\label{eq:pauli-comm}
 \begin{split}
  [\sigma_j, \sigma_k] &= 2\mathrm{i}\epsilon_{jkl}\sigma_l \\
  \{\sigma_j, \sigma_k\} &= 2I \delta_{jk}\quad \text{and} \\
  \sigma_j \sigma_k &= \frac{1}{2}\left([\sigma_j,\sigma_k] +
  \{\sigma_j,\sigma_k\}  \right) = \delta_{jk} I + \mathrm{i} \epsilon_{jkl}
  \sigma_l
,\end{split} 
\end{equation}
with the Levi-Civita tensor $\epsilon_{jkl}$ (where Einstein summation
convention is implied) and the Kronecker delta $\delta_{jk}$. Furthermore, the latter
three are traceless and have eigenvalues of $\pm 1$.
To ease up on the
indices, especially once tensor products of Pauli matrices come into play, one also writes the Pauli matrix with the corresponding capital
letter, $\sigma_x = X, \ldots$. These matrices also form a basis for hermitian
$2\times 2$ matrices. Recall that physical observables are represented by
hermitian matrices. We can therefore consider the Pauli matrices as a basis for
physical observables on qubits. 

While they also play
an important role in representation theory, especially of Lie and Clifford
algebras, they themselves also form a group known as the Pauli group
$\mathcal{P}$. The single-qubit Pauli group is defined as the Pauli matrices
with phases $\pm 1$ and $\pm i$,
\begin{align}
  \mathcal{P} = \{\pm I, \pm i I, \pm X, \pm i X, \pm Y, \pm i Y, \pm Z, \pm i
  Z \}
.\end{align}
This definition can also be generalized to $n$ qubits.
\begin{defn}[Pauli group \cite{gottesmanSurvivingQuantumComputer2024}]\label{defn:pauligroup}
  The Pauli group $\mathcal{P}_n$ is composed of tensor products of $I,\ X,\
  Y,$ and $Z$ on $n$ qubits with an overall phase of $\pm 1$ and $\pm i$.
\end{defn}
We can deduce from \cref{eq:pauli-comm} that $\mathcal{P}_n$ is not Abelian.
The commutation relations can be extended for the tensor products of Paulis,
but there are some cases that then commute non-trivially. For instance,
$X \otimes X \equiv X_1X_2$ commutes with $Z\otimes Z\equiv Z_1Z_2$, even though the individual
Pauli matrices in the tensor products do not commute. These commutation
relations can be compacted into a general statement on tensor products of Pauli
operators. Two $n$-qubit Pauli operators commute non-trivially if there are an
even number of anticommuting pairs in the tensor product structure. This
excludes the identity, of course, since everything commutes trivially with the
identity. The task of finding the commutation relation between two operators
then becomes a counting task.

Another group worth considering is the Clifford group, $\mathsf{C}_n$, which is
often defined as the subgroup of unitary matrices with dimension $2^n$ that
normalize (cf. \cref{defn:normalizer}) the $n$-qubit Pauli group,
$$\mathsf{C}_n = \{ u \in U\left(2^n\right) \mid u^\dagger \mathcal{P}_n u
= \mathcal{P}_n \}.$$
Instead of being a finite group the Clifford group defined in this way is
infinite, since it includes all matrices of the form $u= e^{i\varphi} I$ with
some phase $\varphi \in \mathbb{R}$. By defining it in terms of a finite
subgroup of the above definition, the physical significance of the Clifford
group also becomes apparent.

\begin{defn}[Clifford gates \cite{gottesmanSurvivingQuantumComputer2024}]\label{defn:cliffords}
  The Clifford group is the group generated by the Hadamard, Phase and CNOT
  gates. These are called the Clifford gates. 
\end{defn}

The Clifford gates form an important subset of gates in quantum computing and
especially quantum error correction
\cite{calderbankQuantumErrorCorrection1997,calderbankGoodQuantumErrorcorrecting1996,steaneMultipleParticleInterferenceQuantum1996,bennettQuantumCryptographyPublic2020,bennettCommunicationOneTwoparticle1992}.
However, it should be noted that they do not form a universal set
of quantum gates. While the gates in the Clifford group can create entangled
states with the Hadamard and CNOT gates, one needs an additional gate to
achieve universal quantum computation.
\subsection{The stabilizer group}\label{sec:stabilizergroup}

So far we have only examined groups in isolation. Among other things, we have
shown that the group generated by the Clifford gates normalizes the Pauli
group.  However, the major role group theory plays in physics can best be
demonstrated if one considers the action of group elements on other
mathematical objects outside of the group. These could be, for example,
Lagrangians, or more relevant for us, state vectors. Consider the
two-qubit state vector $\ket{+} = \left( \ket{0} + \ket{1} \right) / \sqrt{2}$.
As an eigenstate of the Pauli operator $X$, we can tell that this state is
resistant to bitflips.  The group-theoretic way to put this is to say that
$\ket{+}$ is $\mathbb{Z}_2$-symmetric. This notion of symmetry is where group
theory finds most of its utility in physics.  As an additional example,
consider the 2-qubit Bell state

\begin{align}
  \ket{\phi} = \frac{\ket{00} + \ket{11}}{\sqrt{2}} 
.\end{align}

Note that the unitary operations $X_1 X_2$ and $Z_1 Z_2$ both have $\ket{\phi}$
as eigenstate with eigenvalue $+1$. Since $$X_1X_2Z_1Z_2\ket{\phi} = \ket{\phi} =
Z_1Z_2X_1X_2\ket{\phi},$$ we can also see how these two operators commute with each
other. The operators $X_1X_2$ and $Z_1Z_2$ are then said to \emph{stabilize} the
state $\ket{\phi}$. It is easy to convince oneself that these operations
stabilizing the state $\ket{\phi}$ should form a group. Doing nothing, i.e. the
identity $I$, clearly stabilizes the state, and since the Pauli matrices square
to the identity, each element of this stabilizer group is its own inverse (we
omit verifying associativity, as this is inherited from $\mathcal{P}_n$). The
matrices $X_1X_2$ and $Z_1Z_2$ therefore generate a symmetry group of
$\ket{\phi}$, since their product is clearly also a symmetry transformation on
$\ket{\phi}$. Therefore, the final concept of group theory we introduce is that
of a symmetry group.

\begin{defn}[Symmetry group]\label{defn:fixpointgroup}
  Let $G$ be a group acting on a set $M$. Let $a\in M$. We then call the
  subgroup
  \[ H = \left\{ h \in G \mid ha = a \right\} \leq G \]
  \emph{symmetry group} or \emph{fixpoint group} of $a$.
\end{defn}

In our example, we have the group of $2$-qubit Pauli matrices, $\mathcal{P}_2$,
acting on the $2$-qubit Hilbert space $H^{\otimes 2}$, with a subgroup of this
group, $\mathcal{S} = \{I, X_1X_2, -Y_1Y_2, Z_1Z_2\} \leq \mathcal{P}_2$, being
the symmetry group of the state $\ket{\phi} \in H^{\otimes 2}$.
%The symmetry group of
%$\ket{\phi} \in H^{\otimes 2}$ is the subgroup of Pauli matrices that have the
%state as eigenvector with eigenvalue $+1$. 
In general, we say that a unitary $U$ stabilizes a pure state $\ket{\psi}$ if
$U\ket{\psi} = \ket{\psi}$. In other words, the stabilizer group of a pure
state
$\ket{\psi}$ is the set of all unitaries that have $\ket{\psi}$ as eigenvector
with eigenvalue $+1$. For all further considerations we restrict the unitaries
to Pauli operators. Thus, the formal definition of an $n$-qubit stabilizer
group can be stated as follows.

\begin{defn}[Stabilizer group]\label{defn:stabilizergroup}
  Let $H^{\otimes n}$ denote the $n$-qubit Hilbert space. Given a subset $V
  \subseteq H^{\otimes n}$, the stabilizer is defined as
  \begin{align}
    \mathcal{S}_V = \{ g \in \mathcal{P}_n \mid g \ket{\phi} = \ket{\phi} \
    \forall \ket{\phi} \in V \} \leq \mathcal{P}_n
  .\end{align}
\end{defn}

It would, in principle, be possible to define stabilizer groups of all
unitaries instead of the Pauli group. We will later see, however, that this
restriction leads to an important result, namely \cref{thm:gottesman-knill}.

We note that global phase matters here. The operators with prefactor, such as
$-I$ are not in the stabilizer. Furthermore, we have that the stabilizer is an
Abelian subgroup of $\mathcal{P}_n$, which can be shown by generalizing the
example above. We can show the necessity of this condition in the following.
Suppose $\ket{\phi}\in V_S$ is non-zero, and $M$ and $N$ are in $\mathcal{S}$. Since $M$
and $N$ are tensor products of Pauli matrices, they either commute or
anticommute. If they anticommute we have
\begin{align}
  \ket{\phi} = MN\ket{\phi} = -NM\ket{\phi} = -\ket{\phi}
,\end{align}
leading to a contradiction, since we had $\ket{\phi}$ being non-zero. (By the
same argument we can rule out $-I$ to be in the stabilizer.)

At this point, we need to be careful not to put the cart before the horse. The
stabilizer group is not the stabilizer of $V$ as such. If that were the case,
then $V = \{ \ket{00}, \ket{11}\}$ would be stabilized by $X_1X_2$. Rather,
$V_S$ is the intersection of subspaces spanned by the eigenvalue $+1$
eigenspaces of the operators in $\mathcal{S}$. What this means in practice is
that when working with the stabilizer formalism, we would much rather first
write out an Abelian subgroup of the Pauli group (without $-I$) and then deduce
the subspace stabilized by this subgroup. 

\begin{defn}[Stabilized state space]\label{defn:stab-statespace}
  Let $\mathcal{S} \leq \mathcal{P}_n$ be Abelian with $-I \not\in
  \mathcal{S}$. The space of states stabilized by $\mathcal{S}$ is
  \begin{align}
    V_S = \{ \ket{\phi} \mid g\ket{\phi} = \ket{\phi} \ \forall g \in
    \mathcal{S} \}
  .\end{align}
  In the literature this space
  is also sometimes equivalently referred to as the code space of the
  stabilizer.
\end{defn}

So far, it is not entirely obvious how keeping track of operators growing
exponentially in size is a worthwhile method of representing quantum states or
subspaces of a larger state space. However, we can simplify the problem
drastically by realizing two facts.

The first is summarized in \cref{defn:generators,thm:maxsize-generators}.
Recall that we can equivalently write a finite group as a collection of at most
$\log(\mathrm{ord}(G))$ generators. The Bell state example from above has a
full stabilizer group of $\mathcal{S} = \{I, X_1X_2, -Y_1Y_2, Z_1Z_2\}$. With
\cref{eq:pauli-comm} we can infer that $X_1X_2\cdot Z_1Z_2 = -Y_1Y_2$, and
since all of them square to the identity, an equivalent form of the stabilizer
group is given by the generating set $G = \langle X_1X_2, Z_1Z_2\rangle$.
At this point we remark that while the generating set is explicitly not the entire group, the
distinction between the two is kept rather loosely. Oftentimes we will write
that some stabilizer group $\mathcal{S}$ is equal to its generating set. This is mostly a matter of
convenience and readability. If context does not explicitly demand it, we write
the generating set and group interchangeably.

The second fact is a more subtle one. It uses the commutation relations
laid out in \cref{eq:pauli-comm}. If we briefly neglect the phases again, or keep
track of them separately, we can write $Y=XZ$. As such, we can encode the
entire stabilizer group in a bit matrix, where $I \equiv 00$, $Z \equiv 01$, $X
\equiv 10,$ and $Y \equiv 11$. The bit matrix of the Bell state is
\begin{align}
  \mathcal{S} \equiv \mqty[1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1]
.\end{align}
This check matrix has a multitude of different use cases, one of them being the
focus of the entirety of \cref{ch:mixed}.
Another one is that if the rows of the
check matrix are linearly independent (mod 2), we have a minimal generating
set. The generators of such a minimal generating set are then called
independent. 

\subsubsection{Stabilizer density matrix}
Before we go on to discuss the dynamics in the stabilizer formalism, i.e.
stabilizer circuits, we define the density matrix for stabilizers. We know that
a stabilizer group $\mathcal{S}$ consists of Pauli operators, which share eigenstates. 
Thus, the density matrix of a pure state $\rho = \dyad{\phi}$ is a
projector onto all $+1$ eigenstates of the Pauli operators in the stabilizer.
This can be constructed by multiplying the projectors onto $+1$ eigenstates of
the generating set. This follows from $\mathbb{P}^2 = \mathbb{P}$ and the fact
that the generating set recovers the whole group with multiplication. Since we
can write projectors of Pauli operators $g$ as $\mathbb{P} = \frac{1}{2}(I+g)$,
we can write
\begin{align}
  \rho = \frac{1}{2^n} \prod_{i=1}^n (I + g_i)
\end{align}
for the density matrix of a pure state.

A mixed state in the stabilizer formalism can be described by removing
generators from the generating set. 
\begin{defn}[Stabilizer density matrix]\label{defn:stab-dmat}
  Let $\mathcal{S}$ be an $n$-qubit stabilizer group with generating set
  $\langle g_1, \ldots, g_l \rangle$ with $0 \leq l \leq n$. The density matrix
  corresponding to this stabilizer group is given by
  \begin{align}\label{eq:densitymatrix-proj-defn}
    \rho = \frac{1}{2^n} \prod_{i=1}^l (I + g_i)
  \end{align}
  or alternatively as
  \begin{align}\label{eq:densitymatrix-sum-defn}
    \rho = \frac{1}{2^n} \sum_{s \in \mathcal{S}} s
  .\end{align}
\end{defn}
We can convince ourselves that this definition is consistent with the general
properties we demand density matrices to fulfil. 
\begin{enumerate}
  \item $\Tr[\rho] = 1$, which is given by the fact that every matrix in the
    product of \cref{eq:densitymatrix-proj-defn} or the sum of
    \cref{eq:densitymatrix-sum-defn} is either a traceless Pauli matrix, or the
    identity of dimension $2^n$. The trace of the latter thus gets absorbed by
    the leading factor.
  \item $\Tr[\rho^2] \leq 1$ is given by the fact that $\rho = c\mathbb{P}_S$
    with $c\leq 1$, and $\mathbb{P}_S$ the projector onto the code space of the
    stabilizer group. We have
    \begin{align}
       \Tr[\rho^2]=\Tr[c^2 \mathbb{P}^2_S] =\Tr[c^2\mathbb{P}_S]
        \leq \Tr[c\mathbb{P}_S] = \Tr[\rho] = 1
    .\end{align}
  \item $\rho = \rho^\dagger$, which is given by the hermiticity of the Pauli
    matrices.
\end{enumerate}

\subsubsection{Entropy of entanglement}

With the notion of a density matrix defined in the stabilizer formalism, we can
ask the question about entropic quantities. Since we introduce methods to
compute other entropic and information-theoretic quantities in
\cref{sec:rel-ent-stab}, we will here only define the entropy of entanglement.
Its definition can be stated as follows.
\begin{defn}[Entropy of entanglement]\label{defn:entent}
  Let $\ket{\phi}\in H^{\otimes N}$ be a bipartite pure state with subsystems
  $A$ and $B$. The entropy of entanglement of $\ket{\phi}$ then reads
  \begin{align}
    %&S_E : H^{\otimes N} \to [0, N/2]\nonumber\\
    S_\mathrm{E}\left(\ket{\phi}\right) = - \Tr[\rho_B \log \rho_B] =
    S_\mathrm{vn}(\rho_B),
  \end{align}
  where $\rho_B = \Tr_A[\dyad{\phi}]$ is the reduced density matrix of subsystem
  $B$ and $S_\mathrm{vn}(\rho_B)$ is the von Neumann entropy, where one usually
  drops the index \enquote{vn} when referring to the von Neumann entropy. Conventionally, one uses the logarithm of base 2.
\end{defn}

In bipartite states this quantity measures how entangled one subsystem is with
the other. That is, it gives a numerical value to the non-local correlations between
subsystems. If we split our stabilizer group into local subgroups of $A$ and
$B$, $\mathcal{S}_A$ and $\mathcal{S}_B$, we have the possibility of another
subgroup remaining, namely $\mathcal{S}_{AB}$ accounting for correlations.
$\mathcal{S}_{A(B)}$ are subgroups containing only operators acting on $A (B)$.
If we split our stabilizer group in this way, we find that there is a nice
closed-form expression for the entanglement entropy
\cite{fattalEntanglementStabilizerFormalism2004}.

\begin{thm}[Entropy of entanglement -- stabilizer]\label{thm:entent}
  The entropy of entanglement of a bipartite pure state $\ket{\phi}_{AB}$ with
  partitions $A$ and $B$ is given
  by
  \begin{align}\label{eq:entent}
    S_E(\ket{\phi}) = \frac{1}{2}\abs{\mathcal{S}_{AB}}
  .\end{align}
\end{thm}
For
the 2-qubit Bell state we once again have $\mathcal{S} = \langle XX,
ZZ\rangle$. Notice that $\mathcal{S}_A = \mathcal{S}_B = \{I\}$ and $\mathcal{S}_{AB} =
\mathcal{S}$. 
The entropy of entanglement between $A$ and $B$ should be $1$
since there is only one independent entangled pair. We could additionally compute
the reduced density matrices and the matrix logarithm instead, arriving at the
same result.

With \cref{eq:entent} we find that this result is also obtained by counting the
generators in $\mathcal{S}_{AB}$, which is $2$, then dividing by $2$, which
gives the correct result of $1$. 

The proof of \cref{thm:entent} and the practical computational implementation
are rather involved. The interested reader is invited to consult ref.
\cite{fattalEntanglementStabilizerFormalism2004}, where the method is
introduced. For now, it suffices to say that the generating set can be brought
into a canonical form, consisting of two sets generating $A$ and $B$,
respectively, and two sets of generators, which anticommute in pairs, that
generate $AB$. The size of this last set is the one which gives the
entanglement entropy.

\subsection{Stabilizer circuits}

Finally, we discuss the dynamics of stabilizer states in quantum circuits. This
is where we can see the main advantages of the stabilizer formalism, as it will
ultimately lead to the efficient simulation of a wide class of quantum circuits
with classical computers. 
As introductory note we define a stabilizer circuit to be a quantum
circuit using only Clifford gates and measurement gates of Pauli operators. Let
us therefore begin by discussing unitary gates.

\subsubsection{Unitary gates}
Suppose we apply a unitary $U$ to a vector space $V_S$ stabilized by
$\mathcal{S}$. For any $\ket{\phi} \in V_S$ and any $g \in \mathcal{S}$ we have
\begin{align}
  U\ket{\phi} = U g \ket{\phi} = U g \underbrace{U^\dagger U}_{I} \ket{\phi}
.\end{align}
We thus have that the state $U\ket{\phi}$ is stabilized by the operator
$UgU^\dagger$. Since our choices of $\ket{\phi}$ and $g$ were arbitrary in
$V_S$ and $\mathcal{S}$, respectively, we have that the transformed vector
space is stabilized by the conjugated stabilizer group
\begin{align}
  U\mathcal{S}U^\dagger = \{ UgU^\dagger \mid g \in \mathcal{S}\}
.\end{align}
Since we restricted ourselves to unitary gates in the Clifford group, which
normalizes the Pauli group, we still exclusively have Pauli operators in
$USU^\dagger$. Our task then becomes to track the effects of Clifford group
elements on a subset of Pauli operators. In particular, we conjugate the
generators in the generating set with the unitary operation corresponding to
the applied gate. Let us therefore consider the
conjugation of the Pauli matrices with the unitary Clifford gates.
For the Hadamard gate $H = \frac{X+Z}{\sqrt{2}}$ we have
\begin{align}
  HXH^\dagger = Z, \quad HYH^\dagger = -Y, \quad HZH^\dagger = X
\end{align}
For the CNOT gate $U$ with qubit 1 as control and qubit 2 as target we have
\begin{alignat}{2}
  UX_1U^\dagger &= X_1X_2, \quad &&UX_2U^\dagger = X_2 \label{eq:cnot-x}\\
  UZ_1U^\dagger &= Z_1, \quad &&UZ_2U^\dagger = Z_1Z_2
,\end{alignat}
where for all other two qubit Pauli operators we can use the above relations
and \cref{eq:pauli-comm} to
deduce their conjugation relations. Furthermore, we already know an efficient method
of tracking generating sets through $X$ and $Z$ alone.
Lastly, we want to consider the action of the phase gate on the Pauli
operators,
\begin{align}
  SXS^\dagger = Y \quad SZS^\dagger = Z
.\end{align}

Note that the Clifford gates being the normalizer of the Pauli group means that
we will inevitably lose gates, which would realize universal quantum computing.
Notable examples of gates not included in the gate set generated by the
Clifford group are the $T$ gate, also known as $\pi /8$ gate, and the Toffoli
gates.

\subsubsection{Measurements}

We now know the mechanisms behind unitary gates in the stabilizer formalism.
However, we can also include measurement gates in quantum circuits. It turns
out that measurements can also be described in a simple way in the stabilizer
formalism. Since physical observables are represented by hermitian operators,
we can assume that the measurement operator is a product of Pauli matrices $M
\in\mathcal{P}_n$. Suppose we measure $M$ in a system in a state $\ket{\phi}$
with stabilizer group $\mathcal{S} = \langle g_1, \ldots, g_n\rangle$. Two
questions naturally arise: what is the measurement result, and how does the
stabilizer group transform under this measurement?

To answer both of these, we first need to realize the two distinct
possibilities:
\begin{enumerate}
  \item $M$ commutes with all the generators of the stabilizer group.
  \item $M$ anticommutes with at least one of the generators. By restructuring
    the generating set, we can ensure that there is at most one generator
    anticommuting with $M$. We thus assume without loss of generality that $M$
    anticommutes with $g_1$.
\end{enumerate}

For the first case, we have that either $M$ or $-M$ is itself an element of the
stabilizer group, since for arbirtary generators $g_j$
\begin{align}
  g_j M \ket{\phi} = M g_j \ket{\phi} = M\ket{\phi}
.\end{align}
As this holds for any $g_j$, we have that $M\ket{\phi}=\pm \ket{\phi}$, where
either $\pm M$ is in the stabilizer group. Assuming w.l.o.g. that
$M\in\mathcal{S}$, we have that the measurement of $M$ yields the result $+1$
with probability one, i.e. deterministically. Since the stabilizer group
already contained $M$, it does not change. 

In the second case, we have that the measurement operator anticommutes with
$g_1$. Note that the projectors for the measurement outcomes are $\mathbb{P} =
\frac{1}{2}\left( I\pm M \right)$. Therefore, the probabilities for the
respective outcomes are given by
\begin{align}
  P(\pm 1) = \Tr[\frac{1}{2}\left( I\pm M \right) \dyad{\phi}]
.\end{align}
With $g_1 \ket{\phi} = \ket{\phi}$ and $\{g_1, M\} =0$ we have
\begin{align}
  P(+1) &= \Tr[\frac{I+M}{2} g_1 \dyad{\phi}] \nonumber\\
        &= \Tr[g_1 \frac{I-M}{2} \dyad{\phi}] \nonumber\\
        &= \Tr[\frac{I-M}{2} \dyad{\phi} g_1] \nonumber\\
        &= \Tr[\frac{I-M}{2} \dyad{\phi}] = P(-1)
.\end{align}
Since we can only measure $+1$ or $-1$ and the probabilities for either are
equal, we can deduce that $P(\pm 1) = 1 / 2$. After the measurement, the
generating set gets affected such that it is now
$\langle \pm M, g_2, \ldots, g_n\rangle$ depending on the measurement outcome.

\subsubsection{Stabilizer circuits and error correction}

No discussion of the stabilizer formalism would be complete without as much as
a mention of its applications in quantum error correction. To this end,
consider the quantum circuit shown in
\cref{fig:error-detection-circuit}. A variation of this circuit will play
an important role in our discussion of the projective transverse-field Ising
model in \cref{sec:intro-ptim}.

\begin{figure}[H]
  \centering
  \input{fig/tikz/entangling-gate.tex}
  \caption{Circuit showing the error detection and correction capabilities
    within the stabilizer formalism.  First, a Bell cluster is created by the
    application of the Hadamard and two CNOT gates.  Afterwards, an error
    occurs in the form of an $X$ measurement, which we can attempt to detect
    via stabilizer measurements.}
  \label{fig:error-detection-circuit}
\end{figure}

Initially, we have the state $\ket{000}$, which is stabilized by $\mathcal{S} =
\langle Z_1, Z_2, Z_3 \rangle$. After applying $H$ to qubit 1 in the circuit,
the stabilizer generators are
conjugated with $H_1$, such that we now have $\mathcal{S} = \langle X_1, Z_2,
Z_3 \rangle$. The corrresponding state stabilized by $\mathcal{S}$ is
$\ket{+}\otimes\ket{00}=\frac{\ket{0}+\ket{1}}{\sqrt{2}} \otimes \ket{00}$.
After the first CNOT, we have $\mathcal{S} = \langle X_1X_2,Z_1Z_2,Z_3\rangle$
according to the transformation rules given in \cref{eq:cnot-x}. Before the measurement on qubit 1, we
apply another CNOT gate, thus creating a 3-qubit Bell cluster
$(\ket{000}+\ket{111}) / \sqrt{2}$ with stabilizer group $\mathcal{S}= \langle
X_1X_2X_3,Z_1Z_2,Z_2Z_3\rangle$. 

Then, an error occurs in the form of an $X$ measurement on qubit 3. The
stabilizer generators are now $\langle X_1X_2, Z_1Z_2, X_3\rangle$, which we
are, in principle, unaware of. However,
since we are performing syndrome measurements afterwards in the form of $ZZ$
measurements, we can detect, and subsequently correct the error. In our
example, a measurement of $Z_1Z_2$ would be a measurement of a stabilizer
generator, and would thus yield $+1$. Next, we measure $Z_2Z_3$. According to
the previously introduced rules, the outcome is $\pm 1$ with probability
$\frac{1}{2}$ and the stabilizer generators afterwards are $\mathcal{S} =
\langle X_1X_2X_3, Z_1Z_2, \pm Z_2Z_3\rangle$. In the case where the outcome is
$+1$, we effectively corrected the error already, since we recovered the
previous stabilizers. In the converse case, we at least detected the syndrome
of the error: bit flip on qubit $3$ must have occurred! We can then correct the
error appropriately and flip it back by means of applying a Pauli-$X$.
%We then measure the first qubit in the
%computational basis. We have that $\{Z_1,X_1\} = 0$, and thus a random outcome
%to the measurement. However, after the measurement we are guaranteed to have
%stabilizers in the form of $\mathcal{S} = \langle \pm Z_1, Z_1Z_2,
%Z_2Z_3\rangle$.
\clearpage
\subsubsection{Simulation of stabilizer circuits}
Simulating stabilizer circuits can thus be done by keeping track of all the
generators and updating them accordingly. As we have seen in above example, we
were not required to perform exponentially hard calculations to get to the
final state of the circuit. This fact can be used to formulate the following
theorem \cite{gottesmanHeisenbergRepresentationQuantum1998}.
%Currently verbatim from \cite{nielsenQuantumComputationQuantum2010}! Beware!
%Actual source is \emph{inside}
%\cite{gottesmanHeisenbergRepresentationQuantum1998}, basically stating "trust
%me bro" (one of the most important theorems of quantum computation is cited as
%private communication in its original source\ldots)
\begin{thm}[Gottesman-Knill theorem]\label{thm:gottesman-knill}
  Suppose a quantum computation is performed which involves only the following
  elements: state preparations in the computational basis, Clifford gates, and
  measurements of observables in the Pauli group (which includes measurement in
  the computational basis as a special case), together with the possibility of
  classical control conditioned on the outcome of such measurements. Such
  computation may be efficiently simulated on a classical computer.
\end{thm}

We will forego a detailed discussion of the simulation of stabilizer circuits
on classical computers, as well as the proof to \cref{thm:gottesman-knill}
until \cref{ch:mixed}. For now we remind ourselves that the Clifford gate set
does not constitute a universal set of quantum gates. Thus, classical computers
\emph{cannot} suffice to efficiently simulate quantum computers

\section{Entanglement Transitions}\label{sec:ent-trans}

Random assortment of MIPT Paper (incomplete):

\begin{itemize}
  \item Fisher paper, das ich mir als allererstes mal durchgelesen hab
    (\citetitle{liMeasurementdrivenEntanglementTransition2019}):
    \cite{liMeasurementdrivenEntanglementTransition2019}
  \item MIPT general, war in \cref{ch:lxe}, aber leider keine ahnung mehr
    warum\ldots (\citetitle{baoTheoryPhaseTransition2020}) \cite{baoTheoryPhaseTransition2020}
  \item \citetitle{baoSymmetryEnrichedPhases2021}
    \cite{baoSymmetryEnrichedPhases2021}
  \item Why not: Measurement induced synchronization von Finn
    (\citetitle{schmolkeMeasurementinducedQuantumSynchronization2023}):
    \cite{schmolkeMeasurementinducedQuantumSynchronization2023}
  \item 2017 WR for quantum state tomography (10 qubits):
    (\citetitle{song10QubitEntanglementParallel2017})
    \cite{song10QubitEntanglementParallel2017}
  \item LXE: Definition: \cite{liCrossEntropyBenchmark2023}; PTIM:
    \cite{tikhanovskayaUniversalityCrossEntropy2023}
  \item Garratt/Altman Paper:
    (\citetitle{garrattProbingPostmeasurementEntanglement2023}) \cite{garrattProbingPostmeasurementEntanglement2023}
  \item self-cite for clout:
    (\citetitle{schmolkeBoostingInformationTransfer2024})
    \cite{schmolkeBoostingInformationTransfer2024}
\end{itemize}

more curated MIPT collection:
\begin{itemize}
  \item Skinner Lecture notes on MIPT
    \cite{skinnerLectureNotesIntroduction2023}
  \item ist zwei mal in meiner zotero library, vielleicht mal auschecken: 
    \cite{hokeMeasurementinducedEntanglementTeleportation2023}
  \item Titel klingt vielversprechend: \cite{baoTheoryPhaseTransition2020}
  \item Ideen von Finn: \cite{joshiObservingQuantumMpemba2024,aresEntanglementAsymmetryProbe2023}
  \item PRX von skinner 1: \cite{skinnerMeasurementInducedPhaseTransitions2019}
    chronologisch das erste
  \item PRX von skinner 2: \cite{nahumMeasurementEntanglementPhase2021}
  \item PRX von skinner 3:
    \cite{nahumEntanglementDynamicsDiffusionannihilation2020}
    Das hat Felix immer auf gemacht um mir das mit den Majoranas zu zeigen
  \item eins noch for good measure:
    \cite{yoshidaDecodingEntanglementStructure2021}
\end{itemize}
%\begin{figure}[H]
%  \centering
%  \input{fig/tikz/intro-hybridcircuit.tex}
%  \caption{Hybrid Circuit Tikz picture??}
%  \label{fig:hybrid-circuit}
%\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics{Untitled.png}
  \caption{Hybrid Circuit Tikz picture??}
  \label{fig:hybrid-circuit}
\end{figure}

\subsection{Phenomenology}
\newpage
\section{The Projective Transverse-Field Ising Model}\label{sec:intro-ptim}
In this section we introduce the model investigated in the main part of the
thesis, known as the projective transverse-field Ising model (abbreviated as
PTIM). We want to give the introduction by going backwards through each of the
concepts constituting the name \enquote{PTIM}, starting with the Ising model.
\subsection{The Ising model}
Hier nur kurz, criticality in 2d, lars onsager whatever, einfach kurz back to
the roots, was sind die origins von dem was folgt
\subsection{The transverse-field Ising model}

The Ising model as conceived by Ernst Ising is a model from classical
statistical mechanics. The spins that constitute it can only take on discrete
values, yes, but they cannot exist in a coherent superposition of up and down,
but only up. For a quantum mechanical analogue to the classical Ising model, we
look towards the \emph{transverse-field} Ising model, which we briefly want to
introduce in this section, following its introduction in ref.
\cite{langLectureNotesTopological2021}.  The transverse-field Ising model is
especially interesting in the context of quantum phases and quantum phase
transitions, where we examine ground states of the Hamiltonian, i.e. at $0$
temperature. The phases and phase transitions are then with respect to
parameters of the Hamiltonian, or more precisely, the coefficients of
non-commuting terms within the Hamiltonian.

In the one-dimensional
case for $L$ spin-$1 /2$ degrees of freedom, the Hamiltonian of the
transverse-field Ising model (TIM) reads
\begin{align}\label{eq:tim-hamiltonian}
  H = -J\sum_{i=1}^L \sigma_i^z \sigma_{i+1}^z - h\sum_{i=1}^L \sigma_i^x
,\end{align}
where we choose $J>0$ (ferromagnetic coupling) and a magnetic field $h>0$. With
positive $h$, the magnetic field it points in $x$-direction, which is transverse to the
$z$-direction, hence the name \emph{transverse-field}. 
Notice that in \cref{eq:tim-hamiltonian}, the terms $\sigma_i^z\sigma_{i+1}^z$
and $\sigma_i^x$ do not commute, $[\sigma_i^z\sigma_{i+1}^z,\sigma_i^x]\neq 0$.
Thus, they do not have a shared eigenbasis, and the ground state of the
Hamiltonian will in general be a superposition of eigenstates of one of the
operators. Conventionally, one chooses the basis of $\sigma_i^z\sigma_{i+1}^z$.
Note that the ferromagnetic coupling implies aligned spins, where nearest
neighbours tend to share directions, while the transverse magnetic field will
flip some spins arbitrarily in the evolution.

If we now consider the two limits of $J \ll h$, i.e. $J\to 0$,  and $h \ll J$,
i.e. $h \to 0$. In the former, we have a weaker coupling between the spins, and
thus a higher tendency for individual spins to flip. The ground state is then
the product state of local ground states of $\sigma_i^x$, $\ket{G_+} =
\ket{++\cdots +}$. This ground state is unique, since each constituent is a
unique ground state. One further feature of it is that is possesses no
long-range order. The spin on site $i$ is completely agnostic to the happenings
of the spin on site $j$, if $i$ and $j$ are sufficiently far apart, and are on
average not aligned (similar to the classical ising model for higher
temperatures). Mathematically, this expresses as 
\begin{align}
\lim_{\abs{i-j}\to\infty} \expval{\sigma_i^z\sigma_j^z}{G_+} \to 0
.\end{align}

The converse case turns out to be rather different. Here, we have that the
coupling is strong, and thus the spins tend to align. Crucially however,
alignment is the only quality of relevance in the context of the ground state,
since the $+1$ eigenvalue of $\sigma_i^z\sigma_{i+1}^z$ is two-fold degenerate
with the eigenstate spanned by $\ket{00}$ and $\ket{11}$. The full Hamiltonian
then inherits this degeneracy, such that its ground state space is spanned by
two states $\ket{G_0} = \ket{00\cdots 0}$ and $\ket{G_1} = \ket{11\cdots 1}$.
The ground state $\ket{G}$ is then an arbitrary coherent superposition of $\ket{G_0}$ and
$\ket{G_1}$,
\begin{align}
  \ket{G} = \alpha \ket{G_0} + \beta \ket{G_1}
.\end{align}
Note that we here do have long-range order, since we are in the
regime, where ferromagnetic coupling is strong. This corresponds to the
0-temperature limit of the classical Ising model. Analogously to the
paramagnetic phase we can write
\begin{align}
  \lim_{\abs{i-j}\to\infty} \expval{\sigma_i^z\sigma_j^z}{G} \to 1
.\end{align}

Note that $\sigma_i^z$ thus corresponds to a local order parameter for the
ferromagnetic phase. Once we were in the paramagnetic regime, we had
$\expval{\sigma_i^z\sigma_j^z}\to 0$ in the long-range limit. A natural
question to follow is, what happens between the regimes of
$h /J \to 0$ and $h /J \to \infty$? The answer is, of course, that there is a phase
transition. Remarkably, this phase transition happens already in the
one-dimensional case, whereas the classical Ising model did not have a phase
transition in one dimension. To understand what happens at the phase
transition, consider the symmetry group of the Hamiltonian, $\mathcal{G}_H =
\{I, X\} \cong \mathbb{Z}_2$ with $X = \prod_i \sigma_i^x$, which
corresponds to flipping all spins. In the paramagnetic phase, the ground state
shares this symmetry, since $X\ket{G_+} = \ket{G_+}$. In the ferromagnetic
phase, we do not have this symmetry in general, since flipping all spins would
transform one ground state into the other. Thus, the symmetry of the
Hamiltonian is spontaneously broken. 
\subsection{The projective transverse-field Ising model}\label{sec:the-ptim}
We now have the baseline set to introduce the \emph{projective}
transverse-field Ising model. Our detailed introduction follows ref.
\cite{langEntanglementTransitionProjective2020}. 
The initial setup of the model is similar to the
TIM, in that we have a linear chain of $L$ spin-$1 /2$ degrees of freedom on sites
$i\leq L$. However, in the PTIM we do not consider time evolution generated by
a Hamiltonian, such as the one in \cref{eq:tim-hamiltonian}, but rather as time
evolution in discrete steps of a quantum circuit consisting of projective
measurements. The action of a measurement of observable $O$ on a state
$\ket{\phi}$ is given by
\begin{align}
  \mathcal{M}[O](\ket{\phi}) =
  \frac{\mathbb{P}_\lambda}{\sqrt{\expval{\mathbb{P}_\lambda}{\phi}}}
\end{align}
with the eigenvalue $\lambda$ of $O$ and probability $P(\lambda;
\ket{\phi}) = \expval{\mathbb{P}_\lambda}{\phi}$.

The connection to the TIM is through the choice of observables $O$ and the
measurement scheme; by choosing
observables $\sigma^x_i$ and $\sigma_i^z\sigma_{i+1}^z$ for each site $i$ and
edge between adjacent sites $e=(i,i+1)$, we have the same non-commuting observables that
constitute the Hamiltonian in \cref{eq:tim-hamiltonian}. Repeated measurements
on the same site thus lead to nontrivial dynamics.

As initial state, we choose the product state of $\ket{+} = (\ket{0} + \ket{1})
/ \sqrt{2}$,
\begin{align}
  \ket{\phi(0)} = \ket{++\cdots +}
.\end{align}
The circuit governing the (discrete) time evolution is then constructed as
follows. (This is also the protocol for circuit generation in upcoming numerical
experiments, i.e. simulations.) For each time step, we go through each site and
set the site variable $x_i$ to 1 with probability $p$, and 0 otherwise.
Independently from this we set the edge variable $z_e=1$ with probability
$1-p$, and 0 otherwise. These vectors correspond to the sites and edges where
$X$ and $ZZ$ measurements will be performed in the circuit. In each timestep we
perform the corresponding measurements, thus evolving the state. An example of
a circuit generated by such a protocol is shown in \cref{fig:ptim-circuit} for
$p=1 /2$. The full circuit defines a (discrete time) quantum trajectory
$\ket{\phi(t)}$.

\begin{figure}[t]
  \centering
  \input{fig/tikz/intro-ptim.tex}
  \caption{Tikz sketch of PTIM setup with $\approx 10$ qubits}
  \label{fig:ptim-circuit}
\end{figure}

With this dynamics of the system, we can probe different quantities, such as
the entanglement entropy $S_E(A)$ of a subsystem $A$, over many trajectories.
Averaging them defines the sample average,
\begin{align}
  X = \frac{1}{M} \sum_{\ket{\phi} \in N} \mathcal{X}(\ket{\phi})
,\end{align}
where $N$ is a set of $M$ randomly generated trajectories. Note that we
typically fix a parameter $p$ and a time $t$ when sampling specific quantities,
where the circuit generation and application defines a classical probability
distribution. 

In spirit of the TIM, we are interested in (sample averaged) quantities as a
function of the probability parameter $p$ characterizing the relative strength
between the non-commuting observables. Varying $p$ in the interval $[0,1]$ is
thus analogous to the limits of $h /J \to 0$ or  $h /J \to \infty$ in the TIM.
As the example most relevant to entanglement transitions, we highlight the
behavior of the entanglement entropy as a function of $p$. This is shown in
\cref{fig:phase-transition}.

\begin{figure}[t]
  \centering
  \includegraphics{Untitled.png}
  \caption{plot of entanglement entropy (and mutual information??) as a
  function of $p$. multiple system sizes, possibly rather large ($\approx 512$
qubits?, maybe $N=\{128,256,512\}$ just for the fun of it). Would need to
include mutual information in the source code.}
  \label{fig:phase-transition}
\end{figure}

As a final note we interpret \cref{fig:phase-transition} in the context of
quantum error correction. The specific choice of observables is not only
interesting in their connection to the TIM.
Recall the circuit shown in
\cref{fig:error-detection-circuit}. If we have an entangled state over multiple
qubits, we can think of $ZZ$ measurements as stabilizer (syndrome) measurements
and projective $X$ erors in the circuit. Due to the projective dynamics,
we not only detect, but also sometimes correct the error. More importantly, the
entanglement structure stays the same no matter what. (This is also manifest in
the fact that the local observables commute with the symmetry operator
$U=\prod_i X_i$, which generates a global $\mathbb{Z}_2$ symmetry.)
For $p\to 0$, we thus
have a lot of effective entangling gates, which are stabilized by sufficiently
frequent $ZZ$ measurements. As a consequence, each qubit is entangled with
every other qubit, giving a half-system entanglement entropy of $1$ (as there
is only one independent Bell pair).

For $p \to 1 /2$ we approach the critical point,
where entanglement clusters are sporadically created and merged together. The
dynamics at this point is nontrivial, and can be characterized by a mapping to
a conformal field theory. Interestingly, the PTIM and its phase transition is
identical to 2d percolation, which is analytically solvable and has a
well-known critical point of $1 /2$. The interested reader is invited to
consult refs.
\cite{aharonyIntroductionPercolationTheory2017,difrancescoConformalFieldTheory1997}
for further details on percolation and conformal field theories, which go
beyond the scope of this work. For now, we can see that the emergence of new
entanglement clusters peaks at $p = 1 /2$.

In the $p\to 1$ regime, we do not stabilize any entanglement, but rather
have a product state of $X$ eigenstates. The more frequent $X$ measurements are
(and conversely, less frequent $ZZ$ measurements), the more we stabilize a
product state.


Hier basically PTIM paper zusammenfassen
\begin{itemize}
  \item PTIM paper: \citetitle{langEntanglementTransitionProjective2020}
    \cite{langEntanglementTransitionProjective2020}
  \item Felix decoder paper: \citetitle{roserDecodingProjectiveTransverse2023}
    \cite{roserDecodingProjectiveTransverse2023}
  \item Colored Cluster Model!
  \item There should be a Skinner paper, but I don't have it in my zotero
    library (yet)
  \item Its \cite{nahumEntanglementDynamicsDiffusionannihilation2020}.
\end{itemize}

\section{Sampling Problem}\label{sec:sampling}
The metaphysics of this endeavour (i.e. this thesis) can
be condensed in the following way; we (a) know from experiments how quantum
systems behave under certain conditions, and (b) predict through theoretical
calculations what these systems might do in another experimental setting. In
the latter case however, there is an uncanny regime of utility, where we either
(a) cannot precisely pass predictions or (b) cannot perform the experiment on
the grounds of hardware limitations\footnote{The Higgs particle was predicted
40 years before it was discovered \textcolor{orange}{citation}} or (c) try to
predict the behavior of quantities not directly measurable. In the case of
quantum computation, and especially in the field of entanglement transitions,
we face these bottlenecks in increasing severity. To make do with them, we
employ classical computer simulations. That is, we perform numerical
experiments. While traditional experiments still serve as the sole proprietor of claim to
ontology, numerical experiments can play a supporting role, 

In the year of our
lord 2024, we phyisicists are thankfully able to perform experiments at home
with cleverly assembled silicon.  That is, nowadays we make do with these
bottlenecks by performing numerical experiments. This is not to discredit the
utility of experiments as such, on the contrary! 

\subsection{Fisher: Linear Cross Entropy}
\cite{liCrossEntropyBenchmark2023}

\subsection{Altman: Upper Bound}
\cite{garrattProbingPostmeasurementEntanglement2024}
